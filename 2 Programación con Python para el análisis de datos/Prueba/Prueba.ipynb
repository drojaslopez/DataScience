{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef30d9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos conectada exitosamente.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'employeeNumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_25416\\4165821089.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m dfResultado2Products= dfResultado2.merge(dfproducts, on=\u001b[33m\"productCode\"\u001b[39m, validate=\u001b[33m\"many_to_one\"\u001b[39m)\n\u001b[32m     99\u001b[39m dfResultado3=dfResultado2Products.drop_duplicates()\n\u001b[32m    100\u001b[39m dfResultado3.to_excel(\u001b[33m\"resuktado3.xlsx\"\u001b[39m, index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    101\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m dfResultado3Employees= dfResultado3.merge(dfemployees, on=\u001b[33m\"employeeNumber\"\u001b[39m, validate=\u001b[33m\"many_to_one\"\u001b[39m)\n\u001b[32m    103\u001b[39m dfResultado4=dfResultado3Employees.drop_duplicates()\n\u001b[32m    104\u001b[39m dfResultado4.to_excel(\u001b[33m\"resuktado4.xlsx\"\u001b[39m, index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    105\u001b[39m \n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10855\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10856\u001b[39m     ) -> DataFrame:\n\u001b[32m  10857\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10858\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10859\u001b[39m         return merge(\n\u001b[32m  10860\u001b[39m             self,\n\u001b[32m  10861\u001b[39m             right,\n\u001b[32m  10862\u001b[39m             how=how,\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'employeeNumber'"
     ]
    }
   ],
   "source": [
    "import psycopg2 \n",
    "import pandas as pd \n",
    "import openpyxl\n",
    "\n",
    "def coneccionBD(): \n",
    "    # Conéctate a PostgreSQL (sin especificar una base de datos) \n",
    "    conn = psycopg2.connect(database=\"classicmodels\", #Es el nombre de la base de datos a la que te quieres conectar, postgres es por defecto \n",
    "                                user=\"postgres\", #Es el nombre de usuario que tiene permisos para conectarse a esa base de datos. \n",
    "                                password=\"postgres\",  \n",
    "                                host=\"localhost\", #Indica dónde está alojada la base de datos. \"localhost\" significa que está en el mismo computador donde estás ejecutando el script de Python. \n",
    "                                port=\"5432\")     \n",
    "    print(\"Base de datos conectada exitosamente.\") \n",
    "    #cursor = conn.cursor()    \n",
    "    # Crear la base de datos si no existe\n",
    "    #cursor.execute(\"DROP DATABASE IF EXISTS northwind2;\")  # Elimina si ya existe\n",
    "    #cursor.execute(\"CREATE DATABASE northwind2;\")  # Crea la base de datos\n",
    "    return conn \n",
    "    \n",
    "def cargarBD(conn):\n",
    "\n",
    "    #cargar el script sql para carga la base de datos\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    sql_file_path = \"classicmodels.sql\"\n",
    "\n",
    "    # Leer el archivo SQL y ejecutarlo\n",
    "    with open(sql_file_path, \"r\", encoding=\"utf-8\") as sql_file:\n",
    "        print(\"Leyendo el archivo SQL...\"+sql_file_path)  \n",
    "        sql_script = sql_file.read()  # Lee todo el contenido del archivo SQL\n",
    "        cursor.execute(sql_script)  # Ejecuta el script\n",
    "    \n",
    "    conn.commit()  # Confirma los cambios\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"Base de datos importada exitosamente.\")\n",
    "    return conn\n",
    "    \n",
    "def obtener_tablas(conn):\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    # Ejecutar una consulta simple\n",
    "    cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='public';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return tables\n",
    "\n",
    "def leer_tabla(conn, tabla):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT * FROM {tabla};\")\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "    cursor.close()\n",
    "    return df\n",
    "\n",
    "    \n",
    " \n",
    "#1. Genera una función llamada leer_tabla(tabla, engine) y utilízala para leer tablas completas desde la base de datos en dataframes independientes. Utilizando esta función, importa las siguientes tablas:\n",
    "#- order\n",
    "#- orderdetails\n",
    "#- customers\n",
    "#- products\n",
    "#- employees\n",
    "\n",
    "\n",
    "conn = coneccionBD()\n",
    "#cargarBD(conn)\n",
    "\n",
    "#tablas=obtener_tablas(conn)\n",
    "\n",
    "dforder=leer_tabla(conn, \"orders\")\n",
    "dforder=dforder.drop_duplicates()\n",
    "dforderdetails=leer_tabla(conn, \"orderdetails\")\n",
    "dforderdetails=dforderdetails.drop_duplicates()\n",
    "dfcustomers=leer_tabla(conn, \"customers\")\n",
    "dfproducts=leer_tabla(conn, \"products\")\n",
    "dfemployees=leer_tabla(conn, \"employees\")\n",
    "\n",
    "#print(dforder)\n",
    "#print(dforderdetails)\n",
    "#print(dfcustomers)\n",
    "#print(dfproducts)\n",
    "#print(dfemployees)\n",
    "\n",
    "\n",
    "# 2. Realiza el cruce entre los DataFrames, asegurándote de utilizar correctamente el parámetro validate para asegurar la integridad referencial.\n",
    "\n",
    "#hacer el cruce entre los dataframes\n",
    "dfjoinOrderOrderDetails= dforder.merge(dforderdetails, on=\"orderNumber\", validate=\"one_to_many\")\n",
    "dfResultado=dfjoinOrderOrderDetails.drop_duplicates()\n",
    "#dfResultado.to_excel(\"resuktado.xlsx\", index=True)\n",
    "\n",
    "dfResultadoCustomers= dfResultado.merge(dfcustomers, on=\"customerNumber\", validate=\"many_to_one\")\n",
    "dfResultado2=dfResultadoCustomers.drop_duplicates()\n",
    "#dfResultado2.to_excel(\"resuktado2.xlsx\", index=True)\n",
    "\n",
    "dfResultado2Products= dfResultado2.merge(dfproducts, on=\"productCode\", validate=\"many_to_one\")\n",
    "dfResultado3=dfResultado2Products.drop_duplicates()\n",
    "dfResultado3.to_excel(\"resuktado3.xlsx\", index=True)\n",
    "\n",
    "dfResultado3Employees= dfResultado3.merge(dfemployees, on=\"employeeNumber\", validate=\"many_to_one\")\n",
    "dfResultado4=dfResultado3Employees.drop_duplicates()\n",
    "dfResultado4.to_excel(\"resuktado4.xlsx\", index=True)\n",
    "\n",
    "\n",
    "#dfjoin2= dfjoin.merge( dfcustomers, on=\"customerNumber\", validate=\"one_to_one\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Agrega las siguientes columnas, considerando su nombre y la fórmula asociada\n",
    "# - venta: quantityOrdered*priceEach\n",
    "# - costo: quantityOrdered*buyPrice\n",
    "# - ganancia: considerando las columnas anteriores\n",
    "# 4. ¿Cuál fue el total de ventas por línea de productos? Incluye una fila de totales.\n",
    "# 5. ¿Cuántos clientes distintos hicieron compras?\n",
    "# 6. ¿Existen clientes que aún no han hecho ninguna compra? ¿Cuántos son?\n",
    "# 7. Se solicita la creación de dos reportes, que respondan las preguntas dadas\n",
    "# ● ¿Cuáles fueron los 10 clientes que reportan mayores ventas brutas en dinero durante el año 2005? Genera un DataFrame y guárdalo en una tabla de Postgre llamada top_10_clientes_2005, en la que se especifique el nombre del cliente y su correspondiente venta, costo y ganancia.\n",
    "# ● ¿Cuál fue el top 10 de artículos más vendidos durante el año 2005? Genera un DataFrame y guárdalo en una tabla de Postgre llamada top_10_prod   uctos_2005, en la que se especifique el nombre del producto y su correspondiente venta, costo y ganancia.  Para este punto debes aplicar el principio DRY, por lo que se deben utilizar funciones parrealizar el filtrado por fechas, generar tablas pivote y escribir tabla en Postgre. Las funciones deben estar en un archivo separado llamado funciones.py y ser importadas al Jupyter Notebook. En este archivo se debe incluir:\n",
    "# ● Una función que permita filtrar un DataFrame por fechas, indicando dataframe, columna para filtrar, fecha inicio y fecha fin. La función debe retornar un DataFrame.\n",
    "# ● Una función que permita generar reportes dependiendo de parámetros de entrada como dataframe, filas, columnas, valores y medida (funcion_agrupadora). Utilizar fill_value = 0. Esta función debe retornar un DataFrame pivotado.\n",
    "# ● Una función que permita escribir en la base de datos a través del guardado de un DataFrame dependiendo de parámetros de entrada como DataFrame, nombre de la tabla, engine y comportamiento en caso de que exista la tabla (if_exists).\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
