{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cc96be",
   "metadata": {},
   "source": [
    "# Desafio Manipulacion y transformacion de datos (Parte I)\n",
    "## Daniel Rojas Lopez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24dc6e5",
   "metadata": {},
   "source": [
    "### Ejercicio 1:\n",
    "\n",
    "Para desarrollar este desafío necesitarás los siguientes archivos:\n",
    "- incidents.pkl\n",
    "- officers.pkl\n",
    "- subjects.pkl\n",
    "\n",
    "a. Carga los datos y crea un DataFrame con cada uno de ellos.\n",
    "\n",
    "b. Genera una tabla que contenga la unión de las 3 tablas. hint: utiliza sufijos para para las columnas que se llaman igual usando el parámetro suffixes de pd.merge().\n",
    "\n",
    "c. Verifica si hay filas duplicadas; si es así, elimínalas.\n",
    "\n",
    "d. ¿Cuántos sujetos de género F hay en el DataFrame resultante? hint: usa el método .value_counts() sobre la columna.\n",
    "\n",
    "e. ¿En cuántos números de caso hay por lo menos una sospechosa que sea mujer? hint: utiliza el método unique() para obtener los valores únicos de una columna específica de un DataFrame luego de filtrar.\n",
    "\n",
    "f. Genera una tabla pivote que muestre en las filas el género del oficial y en las columnas el género del subject. ¿Cómo interpretas los valores que muestra esta vista?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27329cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_1884\\1321931819.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  pkloadIncidents=pkl.load(open('incidents.pkl', 'rb'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "dfMerge Creado \n",
      "\n",
      "dfMerge sin duplicados \n",
      "\n",
      "valorCantidad: \n",
      "gender\n",
      "M    351\n",
      "F     19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "valorCantidad: \n",
      "['M' 'F']\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'gender_officers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m#f. Genera una tabla pivote que muestre en las filas el género del oficial y en las columnas el género del subject. ¿Cómo interpretas los valores que muestra esta vista?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m dfPivot = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfMerge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcase_number\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgender_officers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgender_subjects\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m dfPivot.to_csv(\u001b[33m'\u001b[39m\u001b[33mdfPivot.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:102\u001b[39m, in \u001b[36mpivot_table\u001b[39m\u001b[34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[39m\n\u001b[32m     99\u001b[39m     table = concat(pieces, keys=keys, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m table.__finalize__(data, method=\u001b[33m\"\u001b[39m\u001b[33mpivot_table\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m table = \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m table.__finalize__(data, method=\u001b[33m\"\u001b[39m\u001b[33mpivot_table\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:172\u001b[39m, in \u001b[36m__internal_pivot_table\u001b[39m\u001b[34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[39m\n\u001b[32m    169\u001b[39m     values = \u001b[38;5;28mlist\u001b[39m(values)\n\u001b[32m    171\u001b[39m observed_bool = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default \u001b[38;5;28;01melse\u001b[39;00m observed\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m grouped = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved_bool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    174\u001b[39m     ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouped._grouper.groupings\n\u001b[32m    175\u001b[39m ):\n\u001b[32m    176\u001b[39m     warnings.warn(\n\u001b[32m    177\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe default value of observed=False is deprecated and will change \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto observed=True in a future version of pandas. Specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    182\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:9210\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9213\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9216\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1331\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'gender_officers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl    \n",
    "#a.Carga los datos y crea un DataFrame con cada uno de ellos.\n",
    "\n",
    "pkloadIncidents=pkl.load(open('incidents.pkl', 'rb'))\n",
    "pkloadOfficers=pkl.load(open('officers.pkl', 'rb'))\n",
    "pkloadSubjects=pkl.load(open('subjects.pkl', 'rb'))\n",
    "\n",
    "#convertir los pkl a dataframe\n",
    "\n",
    "dfIncidents=pd.DataFrame(pkloadIncidents)\n",
    "dfOfficers=pd.DataFrame(pkloadOfficers)\n",
    "dfSubjects=pd.DataFrame(pkloadSubjects)\n",
    "\n",
    "#dfIncidents.to_csv('dfIncidents.csv', index=True)\n",
    "#dfOfficers.to_csv('dfOfficers.csv', index=True)\n",
    "#dfSubjects.to_csv('dfSubjects.csv', index=True)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#b. Genera una tabla que contenga la unión de las 3 tablas. hint: utiliza sufijos para para las columnas que se llaman igual usando el parámetro suffixes de pd.merge().\n",
    "dfMerge = pd.merge(dfIncidents, dfOfficers, on='case_number', suffixes=('_incidents', '_officers'))\n",
    "print(\"dfMerge Creado \"+\"\\n\")\n",
    "\n",
    "#c. Verifica si hay filas duplicadas; si es así, elimínalas.\n",
    "dfMerge.drop_duplicates()\n",
    "print(\"dfMerge sin duplicados \"+\"\\n\")\n",
    "\n",
    "#Copiar en archivo csv para visualizar\n",
    "dfMerge.to_csv('dfMerge.csv', index=True)\n",
    "\n",
    "#d. ¿Cuántos sujetos de género F hay en el DataFrame resultante? hint: usa el método .value_counts() sobre la columna.\n",
    "\n",
    "valorCantidad=dfMerge['gender'].value_counts()\n",
    "print(\"valorCantidad: \"+\"\\n\"+ str(valorCantidad))\n",
    "print(\"\\n\")\n",
    "\n",
    "#e. ¿En cuántos números de caso hay por lo menos una sospechosa que sea mujer? hint: utiliza el método unique() para obtener los valores únicos de una columna específica de un DataFrame luego de filtrar.\n",
    "#valorCantidad=dfMerge['gender'].unique()\n",
    "#print(\"valorCantidad: \"+\"\\n\"+ str(valorCantidad))\n",
    "#print(\"\\n\")\n",
    "\n",
    "#f. Genera una tabla pivote que muestre en las filas el género del oficial y en las columnas el género del subject. ¿Cómo interpretas los valores que muestra esta vista?\n",
    "\n",
    "dfPivot = pd.pivot_table(dfMerge, values='case_number', index='gender_officers', columns='gender_subjects', aggfunc='count')\n",
    "dfPivot.to_csv('dfPivot.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc971bb",
   "metadata": {},
   "source": [
    "### Ejercicio 2: \n",
    "Para continuar con el desarrollo de este desafío, necesitarás el archivos\n",
    "Cleaned_DS_Jobs.csv\n",
    "a. Carga los datos y crea un DataFrame con ellos.\n",
    "b. Utiliza la siguiente lista de valores que serán considerados como nulos:\n",
    "[\"na\", \"NA\", -1, \"0\", \"-1\", \"null\", \"n/a\", \"N/A\", \"NULL\"]\n",
    "(hint: utiliza el método replace para reemplazar los valores indicados por np.nan)\n",
    "c. Elimina todas las filas con datos faltantes. (hint: utiliza el método .dropna())\n",
    "d. A partir de la columna “Salary Estimate”, genera dos columnas: Salario Estimado Mínimo\n",
    "y Máximo. (hint: Utiliza el método apply sobre la columna.)\n",
    "e. Realiza la recodificación de la columna Size con los valores de la siguiente tabla: (hint:\n",
    "utilice reemplazo con diccionario usando el método replace sobre la columna.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5682dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Job Title Salary Estimate  \\\n",
      "0                               Sr Data Scientist        137-171    \n",
      "32   Senior Research Statistician- Data Scientist         75-131    \n",
      "38                  Senior Analyst/Data Scientist         75-131    \n",
      "45                          Senior Data Scientist         75-131    \n",
      "54                          Senior Data Scientist         75-131    \n",
      "..                                            ...             ...   \n",
      "579                          Senior Data Engineer        138-158    \n",
      "583                         Senior Data Scientist         80-132    \n",
      "601                         Senior Data Scientist         87-141    \n",
      "640                         Senior Data Scientist        105-167    \n",
      "641        Sr. ML/Data Scientist - AI/NLP/Chatbot        105-167    \n",
      "\n",
      "                                       Job Description  Rating  \\\n",
      "0    Description\\n\\nThe Senior Data Scientist is re...     3.1   \n",
      "32   Acuity is seeking a Senior Research Statistici...     4.8   \n",
      "38   At Edmunds were driven to make car buying easi...     3.4   \n",
      "45   Klaviyo is looking for Senior Data Scientists ...     4.8   \n",
      "54   Benson Hill empowers innovators to develop mor...     3.5   \n",
      "..                                                 ...     ...   \n",
      "579  Senior Data Engineer\\n\\nMaster’s degree in Inf...     2.9   \n",
      "583  Job Requisition ID #\\n20WD40666\\nJob Title\\nSe...     4.0   \n",
      "601  Secure our Nation, Ignite your Future\\n\\nJob S...     4.2   \n",
      "640  About Us\\n\\nAt GutCheck, we pioneered agile ma...     3.8   \n",
      "641  Sr. ML/Data Scientist - AI/NLP/Chatbot\\n\\nMist...     3.8   \n",
      "\n",
      "           Company Name           Location      Headquarters  \\\n",
      "0           Healthfirst       New York, NY      New York, NY   \n",
      "32     Acuity Insurance      Sheboygan, WI     Sheboygan, WI   \n",
      "38          Edmunds.com   Santa Monica, CA  Santa Monica, CA   \n",
      "45              Klaviyo         Boston, MA        Boston, MA   \n",
      "54          Benson Hill    Saint Louis, MO   Saint Louis, MO   \n",
      "..                  ...                ...               ...   \n",
      "579  Affinity Solutions       New York, NY      New York, NY   \n",
      "583            Autodesk  San Francisco, CA    San Rafael, CA   \n",
      "601             ManTech     Alexandria, VA       Herndon, VA   \n",
      "640            GutCheck         Denver, CO        Denver, CO   \n",
      "641    Juniper Networks      Cupertino, CA     Sunnyvale, CA   \n",
      "\n",
      "                        Size       Type of ownership  \\\n",
      "0     1001 to 5000 employees  Nonprofit Organization   \n",
      "32    1001 to 5000 employees       Company - Private   \n",
      "38     501 to 1000 employees       Company - Private   \n",
      "45      201 to 500 employees       Company - Private   \n",
      "54      201 to 500 employees       Company - Private   \n",
      "..                       ...                     ...   \n",
      "579      51 to 200 employees       Company - Private   \n",
      "583  5001 to 10000 employees        Company - Public   \n",
      "601  5001 to 10000 employees        Company - Public   \n",
      "640      51 to 200 employees       Company - Private   \n",
      "641  5001 to 10000 employees        Company - Public   \n",
      "\n",
      "                         Industry  ... company_age python  excel  hadoop  \\\n",
      "0              Insurance Carriers  ...        27.0      0      0       0   \n",
      "32             Insurance Carriers  ...        95.0      0      0       0   \n",
      "38                       Internet  ...        54.0      1      1       0   \n",
      "45   Computer Hardware & Software  ...         8.0      0      0       0   \n",
      "54      Biotech & Pharmaceuticals  ...         8.0      1      1       0   \n",
      "..                            ...  ...         ...    ...    ...     ...   \n",
      "579       Advertising & Marketing  ...        22.0      1      0       1   \n",
      "583  Computer Hardware & Software  ...        38.0      1      0       1   \n",
      "601        Research & Development  ...        52.0      1      0       1   \n",
      "640       Advertising & Marketing  ...        11.0      0      0       0   \n",
      "641   Telecommunications Services  ...        24.0      1      1       0   \n",
      "\n",
      "     spark aws  tableau  big_data        job_simp  seniority  \n",
      "0        0   1        0         0  data scientist     senior  \n",
      "32       0   0        0         0  data scientist     senior  \n",
      "38       0   1        1         0  data scientist     senior  \n",
      "45       0   0        0         0  data scientist     senior  \n",
      "54       0   0        0         1  data scientist     senior  \n",
      "..     ...  ..      ...       ...             ...        ...  \n",
      "579      1   0        1         0   data engineer     senior  \n",
      "583      1   0        0         1  data scientist     senior  \n",
      "601      1   0        1         1  data scientist     senior  \n",
      "640      0   0        0         0  data scientist     senior  \n",
      "641      1   1        0         0  data scientist     senior  \n",
      "\n",
      "[80 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# a. Carga los datos y crea un DataFrame con ellos.\n",
    "df = pd.read_csv(\"Cleaned_DS_Jobs.csv\")\n",
    "\n",
    "# b. Utiliza la siguiente lista de valores que serán considerados como nulos: [\"na\", \"NA\", -1, \"0\", \"-1\", \"null\", \"n/a\", \"N/A\", \"NULL\"] (hint: utiliza el método replace para reemplazar los valores indicados por np.nan)\n",
    "df.replace([\"na\", \"NA\", -1, \"0\", \"-1\", \"null\", \"n/a\", \"N/A\", \"NULL\"], np.nan, inplace=True)\n",
    "\n",
    "# c. Elimina todas las filas con datos faltantes. (hint: utiliza el método .dropna())\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# d. A partir de la columna “Salary Estimate”, genera dos columnas: Salario Estimado Mínimo y Máximo. (hint: Utiliza el método apply sobre la columna.)\n",
    "df[\"min_salary\"] = df[\"Salary Estimate\"].apply(lambda x: int(x.split(\"-\")[0].replace(\"$\", \"\").replace(\",\", \"\")))\n",
    "df[\"max_salary\"] = df[\"Salary Estimate\"].apply(lambda x: int(x.split(\"-\")[1].replace(\"$\", \"\").replace(\",\", \"\")))\n",
    "\n",
    "# e. Realiza la recodificación de la columna Size con los valores de la siguiente tabla: (hint:utilice reemplazo con diccionario usando el método replace sobre la columna.)\n",
    "df[\"Size\"] = df[\"Size\"].replace({\"10000+\": \"Large\", \"5000-9999\": \"Medium\", \"1000-4999\": \"Medium\", \"100-999\": \"Small\", \"10-99\": \"Small\", \"0-9\": \"Small\"})\n",
    "#df.to_csv('df.csv', index=True)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
